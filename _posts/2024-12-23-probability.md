---
date: 2024-12-23
title: Basic probability in Mathlib
---

How do I define a probability space and two independent random variables in Lean? Should I use `IsProbabilityMeasure` or `ProbabilityMeasure`?
How do I condition on an event?

This post answers these and other simple questions about how to express probability concepts using Mathlib.

<!-- TEASER_END -->

The code examples will not mention imports and will assume that we `import Mathlib` in a project that depends on Mathlib.
Many probability related notations are defined in the file Probability/Notation.
Including the following two lines at the beginning of a file after the imports is generally a good idea to work with probability:
```lean
open MeasureTheory ProbabilityTheory
open scoped ENNReal
```
The first line opens namespaces, which means that we will be able to omit any `MeasureTheory.` prefix from lemma names. We will likewise omit that prefix in this text.
The second line makes some notations available. We'll talk about that further down.



# Probability spaces and probability measures

$$g=f \varphi_\alpha^{-1}=f \varphi_\beta^{-1} \varphi_\beta \varphi_\alpha^{-1}=h \varphi_\beta \varphi_\alpha^{-1}$$

```lean
import Mathlib.Geometry.Manifold.MFDeriv.Defs
import Mathlib.Geometry.Manifold.Instances.Real
import Mathlib.Analysis.InnerProductSpace.PiL2
import Mathlib.Geometry.Manifold.AnalyticManifold
import Mathlib.Geometry.Manifold.ContMDiff.Atlas
import Mathlib.Geometry.Manifold.MFDeriv.SpecificFunctions

open Manifold

open SmoothManifoldWithCorners

theorem mfderivWithin_congr_of_eq_on_open
  {m n : ‚Ñï} {M N : Type*}
  [TopologicalSpace M]
  [ChartedSpace (EuclideanSpace ‚Ñù (Fin m)) M]
  [SmoothManifoldWithCorners (ùì° m) M]
  [TopologicalSpace N]
  [ChartedSpace (EuclideanSpace ‚Ñù (Fin n)) N]
  [SmoothManifoldWithCorners (ùì° n) N]
  (f g : M ‚Üí N) (s : Set M)
  (ho : IsOpen s)
  (he : ‚àÄ x ‚àà s, f x = g x) :
  ‚àÄ x ‚àà s, mfderivWithin (ùì° m) (ùì° n) f s x = mfderivWithin (ùì° m) (ùì° n) g s x := by
    intros x hy
    exact mfderivWithin_congr (IsOpen.uniqueMDiffWithinAt ho hy) he (he x hy)
```

First, in order to work on probability we need a measurable space.
We can define a probability measure on such a space as follows.
```lean
variable {Œ© : Type*} [MeasurableSpace Œ©] {P : Measure Œ©} [IsProbabilityMeasure P]
```
The class `MeasurableSpace Œ©` defines a sigma-algebra on `Œ©`. We then introduced a measure `P` on that sigma-algebra and specified that it should be a probability measure.
If we want to work on `‚Ñù` or another well known type the typeclass inference system will find `[MeasurableSpace ‚Ñù]` on its own. We can write simply
```lean
variable {P : Measure ‚Ñù} [IsProbabilityMeasure P]
```

With the code above, we can introduce several probability measures on the same space. When using lemmas and definitions about those measures, we will need to specify which measure we are talking about.
For example, the variance of a random variable `X` with respect to the measure `P` will be `variance X P`.

But perhaps we just want a space with a canonical probability measure, which would be the one used without us having to tell Lean explicitly.
That can be done with the `MeasureSpace` class. A `MeasureSpace` is a `MeasurableSpace` with a canonical measure called `volume`.
The probability library of Mathlib defines a notation `‚Ñô` for that measure. We still need to tell that we want it to be a probability measure though.
```lean
variable {Œ© : Type*} [MeasureSpace Œ©] [IsProbabilityMeasure (‚Ñô : Measure Œ©)]
```
Remark 1: in the code above we can't write only `[IsProbabilityMeasure ‚Ñô]` because Lean would then not know to which space the default measure `‚Ñô` refers to.
That will not be necessary when we use `‚Ñô` in proofs because the context will be enough to infer `Œ©`.

Remark 2: a lemma written for `P : Measure Œ©` in a `MeasurableSpace Œ©` will apply for the special measure `‚Ñô` in a `MeasureSpace Œ©`, but the converse is not true.
Mathlib focuses on generality, hence uses the `MeasurableSpace` spelling for its lemmas. In another context, the convenience of `MeasureSpace` may be preferable.


## `IsProbabilityMeasure` vs `ProbabilityMeasure`

The examples above used `{P : Measure Œ©} [IsProbabilityMeasure P]` to define a probability measure. That's the standard way to do it.
Mathlib also contains a type `ProbabilityMeasure Œ©`: the subtype of measures that are probability measures.
The goal of that type is to work on the set of probability measures on `Œ©`.
In particular, it comes with a topology, the topology of convergence in distribution (weak convergence of measures).
If we don't need to work with that topology, `{P : Measure Œ©} [IsProbabilityMeasure P]` should be preferred.


## Probability of events

An event is a measurable set: there is no special event definition in Mathlib.
The probability of that event is the measure of the set.
A `Measure` can be applied to a set like a function and returns a value in `ENNReal` (denoted by `‚Ñù‚â•0‚àû`, available after `open scoped ENNReal`).
```lean
example (P : Measure ‚Ñù) (s : Set ‚Ñù) : ‚Ñù‚â•0‚àû := P s
```
The probability of the event `s` is thus `P s`.
The type `‚Ñù‚â•0‚àû` represents the nonnegative reals and infinity: the measure of a set is a nonnegative real number which in general may be infinite.
If `P` is a probability measure, it actually takes only values up to 1.
The tactic `simp` knows that a probability measure is finite and will use the lemmas `measure_ne_top` or `measure_lt_top` to prove that `P s ‚â† ‚àû` or `P s < ‚àû`.

The operations on `‚Ñù‚â•0‚àû` are not as nicely behaved as on `‚Ñù`: `‚Ñù‚â•0‚àû` is not a ring. For example, subtraction truncates to zero.
If one finds that lemma `lemma_name` used to transform an equation does not apply to `‚Ñù‚â•0‚àû`, a good thing to try is to find a lemma named like `ENNReal.lemma_name_of_something` and use that instead (it will typically require that one variable is not infinite).

For many lemmas to apply, the set `s` will need to be a measurable set. The way to express that is `MeasurableSet s`.


## Random variables

A random variable is a measurable function from a measurable space to another.
```lean
variable {Œ© : Type*} [MeasurableSpace Œ©] {X : Œ© ‚Üí ‚Ñù} (hX : Measurable X)
```
In that code we defined a random variable `X` from the measurable space `Œ©` to `‚Ñù` (for which the typeclass inference system finds a measurable space instance). The assumption `hX` states that `X` is measurable, which is necessary for most manipulations.

If we define a measure `P` on `Œ©`, we can talk about the law or distribution of a random variable `X : Œ© ‚Üí E`.
The law of `X` is a measure on `E`, with value `P (X ‚Åª¬π' s)` on any measurable set `s` of `E`.
This is how we define the map of the measure `P` by `X`, `Measure.map X P` or more succinctly `P.map X`.
There is no specific notation for that law.
To say that `X` is Gaussian with mean 0 and variance 1, write `P.map X = gaussianReal 0 1`.

The expectation of `X` is the integral of that function against the measure `P`, written `‚à´ œâ, X œâ ‚àÇP`.
The notation `P[X]` is shorthand for that expectation. In a `MeasureSpace`, we can further use the notation `ùîº[X]`.

Remark: there are two types of integrals in Mathlib, Bochner integrals and Lebesgue integrals.
The expectation notations stand for the Bochner integral, which is defined for `X : Œ© ‚Üí E` with `E` a normed space over `‚Ñù` (`[NormedAddCommGroup E] [NormedSpace ‚Ñù E]`).
They don't work for `Y : Œ© ‚Üí ‚Ñù‚â•0‚àû` since `‚Ñù‚â•0‚àû` is not a normed space, but those functions can be integrated with the Lebesgue integral: `‚à´‚Åª œâ, Y œâ ‚àÇP`.
There is no expectation notation for the Lebesgue integral.

## Discrete probability

In discrete probability, measurability is not an issue: every set and every function are measurable.
The typeclass `[DiscreteMeasurableSpace Œ©]` signals that every set of `Œ©` is measurable and the lemma `MeasurableSet.of_discrete` provides a proof of measurability.
To obtain measurability of a function from `Œ©`, use `Measurable.of_discrete`.

Any countable type with measurable singletons is a `DiscreteMeasurableSpace`, for example `‚Ñï` or `Fin n`.

A way to define a probability measure on a discrete space `Œ©` is to use the type `PMF Œ©`, which stands for probability mass function.
`PMF Œ©` is the subtype of functions `Œ© ‚Üí ‚Ñù‚â•0‚àû` that sum to 1.
One can get a `Measure Œ©` from `p : PMF Œ©` with `p.toMeasure`.
When writing a theorem about probability on finite spaces, it preferable to write it for a `Measure` in a `DiscreteMeasurableSpace` than for a `PMF` for better integration with the library.


## Additional typeclasses on measurable spaces

Some results in probability theory require the sigma-algebra to be the Borel sigma-algebra, generated by the open sets. For example, with the Borel sigma-algebra the open sets are measurable and continuous functions are measurable.
For that we first need `Œ©` to be a topological space and we then need to add a `[BorelSpace Œ©]` variable.
```lean
variable {Œ© : Type*} [MeasurableSpace Œ©] [TopologicalSpace Œ©] [BorelSpace Œ©]
```

For properties related to conditional distributions, it is often convenient or necessary to work in a standard Borel space (a measurable space arising as the Borel sets of some Polish topology). See the `StandardBorelSpace` typeclass.
Note that a countable discrete measurable space is a standard Borel space, so there is no need to worry about that typeclass when doing discrete probability.


## Various probability definitions

This section contains pointers to the Mathlib definitions for several probability objects.
That list might be out of date when you read this! Look around in the [documentation](https://leanprover-community.github.io/mathlib4_docs/).

### CDF, pdf, variance, moments

Here is a list of common concepts about real probability distributions.

- Probability density function of a random variable `X : Œ© ‚Üí E` in a space with measure `P : Measure Œ©`, with respect to measure `Q : Measure E`: `pdf X P Q`.
- Cumulative distribution function of `P : Measure ‚Ñù`: `cdf P`. This is a `StieltjesFunction`, a monotone right continuous real function.
- Expectation of `X` under `P`: `P[X]`.
- Variance: `variance X P`.
- Moment of order `p`: `moment X p P`.
- Central moment of order `p`: `centralMoment X p P`.
- Moment generating function of `X` under `P` at `t : ‚Ñù`: `mgf X P t`.
- Cumulant generating function: `cgf X P t`.

### Known probability distributions

The Probability/Distributions folder of Mathlib contains several common probability laws: Exponential, Gamma, Gaussian (only in `‚Ñù`), Geometric, Pareto, Poisson, Uniform.
Other measures that can be used to build probability distributions are defined elsewhere, in the MeasureTheory folder: counting measure, Dirac, Lebesgue measure.

For example, the Gaussian distribution on the real line with mean `Œº` and variance `v` is a `Measure ‚Ñù`:
```lean
def gaussianReal (Œº : ‚Ñù) (v : ‚Ñù‚â•0) : Measure ‚Ñù := -- omitted
```
The definition is followed by an instance `IsProbabilityMeasure (gaussianReal Œº v)` that states that this is a probability measure. It is defined as the Dirac distribution for `v = 0`.
The file where the Gaussian is defined also contains properties of its probability density function.

As another example, to take the uniform distribution on a finite set `s : Set Œ©`, use `uniformOn s : Measure Œ©`.

### Identically distributed

`IdentDistrib X Y P Q` (or `IdentDistrib X Y` in `MeasureSpace`) states that `X` and `Y` are identically distributed.

### Conditioning

#### Conditional probability

The probability of a set `s` conditioned on another set `t` for the measure `P` is `P[s|t]`, which equals `P (s ‚à© t) / P t`.
Conditioning on `t` defines a new measure named `cond P t`, denoted by `P[|t]`. With that notation, `P[s|t] = P[|t] s`.

For `X : Œ© ‚Üí E` and `x : E`, we write `P[|X ‚Üê x]` for the probability measure conditioned on `X = x`. It is notation for `P[|X ‚Åª¬π' {x}]`.
This is meaningful only if `X = x` has non-zero probability, so that definition is mostly useful for discrete probability.

#### Conditional expectation

The conditional expectation of a random variable `Y : Œ© ‚Üí F` given a sigma-algebra `m` of `Œ©` is `condexp m P Y`, with notation `P[Y | m]`. This is a random variable `Œ© ‚Üí F`, and it is `m`-measurable.
The conditional expectation of `Y` given `X : Œ© ‚Üí E` can be obtained with `P[Y | mE.comap X]`, in which `mE : MeasurableSpace E` is the sigma-algebra on `E`.
The sigma-algebra `mE.comap X` (or `MeasurableSpace.comap X mE`) on `Œ©` is the sigma-algebra generated by `X`. It is the smallest sigma-algebra for which all the sets `X ‚Åª¬π' s` are measurable, for `s` ranging over the measurable sets on `E`.

We have special notation for the conditional expectation of the real indicator of a set `s : Set Œ©`: `P‚ü¶s | m‚üß : Œ© ‚Üí ‚Ñù`.

#### Conditional probability distribution

The regular conditional probability distribution of `Y : Œ© ‚Üí F` given `X : Œ© ‚Üí E`, for `F` standard Borel, is denoted by `condDistrib Y X P` (for a measure `P` on `Œ©`).
This is a Markov kernel from `E` to `F` (see further down) such that for all measurable sets `s` of `F`, for `P`-almost every `œâ : Œ©`, `condDistrib Y X P (X œâ) s` is equal to `P‚ü¶Y ‚Åª¬π' s|mE.comap X‚üß` (up to a mismatch in types: `‚Ñù‚â•0‚àû` for `condDistrib` versus `‚Ñù` for the conditional expectation of the indicator).

### Independence

Mathlib has several definitions for independence. We can talk about independence of random variables, of sets, of sigma-algebras.
The definitions also differ depending on whether we consider only two random variables of an indexed family.
Finally, there are also conditional variants of all those definitions.

#### Unconditional independence

Two independent random variables can be defined like this:
```lean
variable {Œ© : Type*} [MeasurableSpace Œ©] {P : Measure Œ©}
  {X : Œ© ‚Üí ‚Ñù} {Y : Œ© ‚Üí ‚Ñï} (hX : Measurable X) (hY : Measurable Y)
  (hXY : IndepFun X Y P)
```
On a measure space, we can write `IndepFun X Y` without the measure argument.

For a family `X : Œπ ‚Üí Œ© ‚Üí ‚Ñù` of independent random variables, use `iIndepFun`.

To express independence of sets, use `IndepSet` (two sets) and `iIndepSet` (family of sets). For sigma-algebras, `Indep` and `iIndep`.

#### Conditional independence

The way to write that two random variables `X : Œ© ‚Üí E` and `Y : Œ© ‚Üí F` are conditionally independent given a sub-sigma-algebra `m` with respect to the measure `P` is `CondIndepFun m hm X Y P`, in which `hm : m ‚â§ mŒ©` is a proof that `m` is a sub-sigma-algebra of the sigma-algebra `mŒ©` of `Œ©`.
We would omit the measure argument in a `MeasureSpace`.
That definition requires that `Œ©` be standard Borel space.

To write that `X : Œ© ‚Üí E` and `Y : Œ© ‚Üí F` are conditionally independent given a third random variable `Z : Œ© ‚Üí G` (with measurability assumption `hZ : Measurable Z`), write that they are independent given the sigma-algebra generated by `Z`.
That is, for `mG : MeasurableSpace G` the sigma-algebra on `G`, write `CondIndepFun (mG.comap Z) hZ.comap_le X Y P`.
As of writing this blog post, there is no shorter spelling of that fact.

For families of functions, use `iCondIndepFun`.
For sets, use `CondIndepSet` (two sets) and `iCondIndepSet` (family of sets). For sigma-algebras, `CondIndep` and `iCondIndep`.

### Stochastic processes, filtrations, martingales

A stochastic process with real values (for example) is a function `X : Œπ ‚Üí Œ© ‚Üí ‚Ñù` from an index set `Œπ` to random variables `Œ© ‚Üí ‚Ñù`.

A filtration on the index set can be defined with `‚Ñ± : Filtration Œπ m`, in which `m` is a sigma-algebra on `Œ©`. `‚Ñ±` is a monotone sequence of sub-sigma-algebras of `m`. The sigma-algebra at index `i` is `‚Ñ± i`.

We can state that a process `X` is adapted to the filtration `‚Ñ±` with `adapted ‚Ñ± X`.
We can write that it is a martingale with respect to `‚Ñ±` and the measure `P : Measure Œ©` with `Martingale X ‚Ñ± P`.

A stopping time with respect to a filtration `‚Ñ±` is a function `œÑ : Œ© ‚Üí Œπ` such that for all `i`, the preimage of `{j | j ‚â§ i}` along `œÑ` is measurable with respect to `‚Ñ± i`. 
`IsStoppingTime ‚Ñ± œÑ` states that `œÑ` is a stopping time.

Remark: there is an issue with the current definition of stopping times, which is that if the index set of the filtration `Œπ` is the set of natural numbers `‚Ñï` then `œÑ` has to take values in `‚Ñï`. In particular it cannot be infinite.
That can cause friction with informal stopping times which are often understood to be infinite when they are not well defined.

### Transition kernels

A transition kernel `Œ∫` from a measurable space `E` to a measurable space `F` is a measurable function from `E` to `Measure F`. That is, for every measurable set `s` of `F`, the function `fun x ‚Ü¶ Œ∫ x s` is measurable.
Transition kernels are represented in Mathlib by the type `Kernel E F`.
A kernel such that all measures in its image are probability measures is called a Markov kernel. This is denoted by `IsMarkovKernel Œ∫`.
There are other typeclasses for kernels that are finite (`IsFiniteKernel`) or s-finite (countable sum of finite, `IsSFiniteKernel`).

Kernels are perhaps more widely used in Mathlib than one would expect. For example independence and conditional independence are both expressed as special cases of a notion of independence with respect to a kernel and a measure.
